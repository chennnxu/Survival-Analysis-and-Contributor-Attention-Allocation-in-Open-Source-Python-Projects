{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col,split,regexp_extract_all,regexp_extract,explode,size,countDistinct, when, max, min, count\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 21:07:16 WARN Utils: Your hostname, cssh-alpha resolves to a loopback address: 127.0.1.1; using 134.130.186.164 instead (on interface bond0)\n",
      "24/01/29 21:07:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/29 21:07:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"REMDataAnalysis\").master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.meomory\", \"40g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # .config(\"spark.executor.meomory\", \"24g\") \\\n",
    "    #     .config(\"spark.driver.maxResultSize\", \"16g\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- message: binary (nullable = true)\n",
      " |-- author: binary (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- date_offset: short (nullable = true)\n",
      " |-- committer: binary (nullable = true)\n",
      " |-- committer_date: timestamp (nullable = true)\n",
      " |-- committer_offset: short (nullable = true)\n",
      " |-- directory: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------------+----------------+--------------------+\n",
      "|                  id|             message|              author|               date|date_offset|           committer|     committer_date|committer_offset|           directory|\n",
      "+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------------+----------------+--------------------+\n",
      "|e0b7070276027464e...|[66 69 78 69 6E 6...|[EE 2C E1 74 91 2...|2018-10-23 21:01:57|        120|[EE 2C E1 74 91 2...|2019-01-10 23:06:25|              60|e56f5ce1308377acc...|\n",
      "+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------------+----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Length of the dataset: 5971345 rows\n",
      "root\n",
      " |-- revision: string (nullable = true)\n",
      " |-- snapshot: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|            revision|            snapshot|\n",
      "+--------------------+--------------------+\n",
      "|1cace1fca5b7b23c6...|270132c1ea02dde03...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 76359551 rows\n",
      "root\n",
      " |-- origin: string (nullable = true)\n",
      " |-- visit: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- snapshot: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+--------------------+-------+--------------------+\n",
      "|              origin|visit|                date| status|            snapshot|\n",
      "+--------------------+-----+--------------------+-------+--------------------+\n",
      "|deb://Debian/pack...|  287|2017-12-22 16:34:...|partial|48cfa88294968d0de...|\n",
      "+--------------------+-----+--------------------+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Length of the dataset: 784380 rows\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the folder containing Orc files\n",
    "input_folder_revision = \"/home/cxu/swh-popular-3k-python/revision\"\n",
    "input_folder_snapshot = \"/home/cxu/code-survival-analysis/snapshot_revisions_explored\"\n",
    "# input_folder_release = \"/home/cxu/swh-popular-3k-python/release\"\n",
    "input_folder_origin_visit_status = \"/home/cxu/swh-popular-3k-python/origin_visit_status\"\n",
    "# input_folder_origin_visit = \"/home/cxu/swh-popular-3k-python/origin_visit\"\n",
    "\n",
    "# Read files into a DataFrame\n",
    "revision = spark.read.format(\"orc\").load(input_folder_revision)\n",
    "snapshot = spark.read.format(\"json\").load(input_folder_snapshot)\n",
    "# release = spark.read.format(\"orc\").load(input_folder_release)\n",
    "origin_visit_status = spark.read.format(\"orc\").load(input_folder_origin_visit_status)\n",
    "# origin_visit = spark.read.format(\"orc\").load(input_folder_origin_visit)\n",
    "\n",
    "# Data description\n",
    "# Print the schema of the dataset\n",
    "tables = [revision, snapshot, origin_visit_status]\n",
    "for data in tables:\n",
    "    data.printSchema()\n",
    "    # Show the first 5 rows of the dataset\n",
    "    data.show(1)\n",
    "    # Get the length of the dataset\n",
    "    dataset_length = data.count()\n",
    "    # Show the length of the dataset\n",
    "    print(f\"Length of the dataset: {dataset_length} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = snapshot.dropDuplicates().na.drop()\n",
    "origin_visit_status = origin_visit_status.dropDuplicates().na.drop()\n",
    "revision = revision.dropDuplicates().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns = [\"origin_visit_status.origin\", \"revision.author\", \"revision.date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = revision \\\n",
    "    .join(snapshot, revision[\"id\"] == snapshot[\"revision\"])\\\n",
    "    .join(origin_visit_status, snapshot[\"snapshot\"] == origin_visit_status[\"snapshot\"])\\\n",
    "    .select(origin_visit_status[\"origin\"], revision[\"id\"], revision[\"author\"], revision[\"date\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "|              origin|                  id|              author|               date|\n",
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://gitlab.co...|000271b8d8efd7a00...|[BC DA FA F8 53 0...|2012-06-18 16:51:44|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00033dbf403b08c9c...|[56 17 B0 21 3F B...|2007-08-18 22:05:58|\n",
      "|https://github.co...|00046370c4fbde0a9...|[0B 46 4D A0 BA 7...|2019-01-28 15:47:53|\n",
      "|https://github.co...|00046370c4fbde0a9...|[0B 46 4D A0 BA 7...|2019-01-28 15:47:53|\n",
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropDuplicates().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.select('author').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.toPandas().to_csv('data/python3k_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
